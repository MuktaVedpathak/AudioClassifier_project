{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3333ada6-ec2a-43c8-af35-3601dd4d3e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install librosa\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76cf662f-f5ba-43eb-898e-43517c394741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c085757-dea0-4496-8d84-1bb9c10640b5",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81d644ff-586f-4112-a324-175b242c94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file):\n",
    "    data,sample_rate = librosa.load(file)\n",
    "    mfccs_features = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=40)\n",
    "    mfccs_scaled_features=np.mean(mfccs_features.T, axis=0)\n",
    "\n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f99147-35fa-4a99-b2ea-1663df129db8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3552it [01:08, 58.27it/s]C:\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1323\n",
      "  warnings.warn(\n",
      "8324it [02:35, 65.38it/s]C:\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1103\n",
      "  warnings.warn(\n",
      "C:\\Python312\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1523\n",
      "  warnings.warn(\n",
      "8732it [02:43, 53.34it/s]\n"
     ]
    }
   ],
   "source": [
    "extracted_features=[]\n",
    "\n",
    "for index_num, row in tqdm.tqdm((metadata.iterrows())):\n",
    "    file= os.path.join(os.path.abspath(audio_dataset),'fold'+str(row[\"fold\"])+'/',str (row[\"slice_file_name\"]))\n",
    "    final_class_labels=row[\"class\"]\n",
    "    data=features_extractor(file)\n",
    "    extracted_features.append([data,final_class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80224b67-97df-4d93-9ac5-cd16fd81c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "051076d6-3179-4dac-9fe2-6202ca730d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccb9419d-0f58-43fb-a769-0f05076523df",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da5a2c63-d2c8-4d31-bb51-845f33d39809",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8bc7d-9aa2-4820-9eb6-a68fc012f889",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6211b667-760a-4e97-bffc-2da428acc77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "167af539-b8a2-4b03-9d9a-3ece037ccf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Input(shape=(40,)))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c7e0471-ce91-40f3-b6bb-dd80fddfc273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,100</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,100</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │           \u001b[38;5;34m4,100\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │          \u001b[38;5;34m20,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m20,100\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,010\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,410</span> (177.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m45,410\u001b[0m (177.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,410</span> (177.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m45,410\u001b[0m (177.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f74c099d-3c83-496f-b653-0733e097c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bf119ee-c4ae-4aae-bb97-b6523d078cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1161 - loss: 23.6143\n",
      "Epoch 1: val_loss improved from inf to 2.25927, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1161 - loss: 23.5409 - val_accuracy: 0.1434 - val_loss: 2.2593\n",
      "Epoch 2/100\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1228 - loss: 3.8874\n",
      "Epoch 2: val_loss did not improve from 2.25927\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1228 - loss: 3.8838 - val_accuracy: 0.1175 - val_loss: 2.2830\n",
      "Epoch 3/100\n",
      "\u001b[1m122/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1327 - loss: 2.6014\n",
      "Epoch 3: val_loss did not improve from 2.25927\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1328 - loss: 2.5943 - val_accuracy: 0.1205 - val_loss: 2.2774\n",
      "Epoch 4/100\n",
      "\u001b[1m130/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1257 - loss: 2.3807\n",
      "Epoch 4: val_loss did not improve from 2.25927\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1260 - loss: 2.3799 - val_accuracy: 0.1232 - val_loss: 2.2731\n",
      "Epoch 5/100\n",
      "\u001b[1m130/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1323 - loss: 2.2949\n",
      "Epoch 5: val_loss did not improve from 2.25927\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.1322 - loss: 2.2950 - val_accuracy: 0.1200 - val_loss: 2.2698\n",
      "Epoch 6/100\n",
      "\u001b[1m136/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1362 - loss: 2.3015\n",
      "Epoch 6: val_loss improved from 2.25927 to 2.24099, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.1362 - loss: 2.3012 - val_accuracy: 0.1457 - val_loss: 2.2410\n",
      "Epoch 7/100\n",
      "\u001b[1m118/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1280 - loss: 2.2656\n",
      "Epoch 7: val_loss improved from 2.24099 to 2.23493, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.1303 - loss: 2.2650 - val_accuracy: 0.1496 - val_loss: 2.2349\n",
      "Epoch 8/100\n",
      "\u001b[1m132/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1487 - loss: 2.2535\n",
      "Epoch 8: val_loss improved from 2.23493 to 2.16993, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1487 - loss: 2.2529 - val_accuracy: 0.2423 - val_loss: 2.1699\n",
      "Epoch 9/100\n",
      "\u001b[1m110/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1813 - loss: 2.2110\n",
      "Epoch 9: val_loss improved from 2.16993 to 2.14615, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.1808 - loss: 2.2102 - val_accuracy: 0.2494 - val_loss: 2.1462\n",
      "Epoch 10/100\n",
      "\u001b[1m115/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1823 - loss: 2.2256\n",
      "Epoch 10: val_loss improved from 2.14615 to 2.09025, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1830 - loss: 2.2212 - val_accuracy: 0.2655 - val_loss: 2.0902\n",
      "Epoch 11/100\n",
      "\u001b[1m134/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1947 - loss: 2.1491\n",
      "Epoch 11: val_loss improved from 2.09025 to 2.05422, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.1948 - loss: 2.1494 - val_accuracy: 0.2785 - val_loss: 2.0542\n",
      "Epoch 12/100\n",
      "\u001b[1m118/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1971 - loss: 2.1487\n",
      "Epoch 12: val_loss improved from 2.05422 to 2.02821, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1983 - loss: 2.1487 - val_accuracy: 0.2836 - val_loss: 2.0282\n",
      "Epoch 13/100\n",
      "\u001b[1m123/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2274 - loss: 2.1136\n",
      "Epoch 13: val_loss improved from 2.02821 to 2.00209, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2268 - loss: 2.1132 - val_accuracy: 0.2849 - val_loss: 2.0021\n",
      "Epoch 14/100\n",
      "\u001b[1m121/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2178 - loss: 2.0955\n",
      "Epoch 14: val_loss improved from 2.00209 to 1.92128, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2186 - loss: 2.0937 - val_accuracy: 0.3019 - val_loss: 1.9213\n",
      "Epoch 15/100\n",
      "\u001b[1m121/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2334 - loss: 2.0414\n",
      "Epoch 15: val_loss improved from 1.92128 to 1.88126, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2337 - loss: 2.0400 - val_accuracy: 0.3333 - val_loss: 1.8813\n",
      "Epoch 16/100\n",
      "\u001b[1m134/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2427 - loss: 2.0342\n",
      "Epoch 16: val_loss improved from 1.88126 to 1.80830, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2431 - loss: 2.0333 - val_accuracy: 0.3433 - val_loss: 1.8083\n",
      "Epoch 17/100\n",
      "\u001b[1m136/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2720 - loss: 1.9691\n",
      "Epoch 17: val_loss improved from 1.80830 to 1.75954, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.2721 - loss: 1.9688 - val_accuracy: 0.3825 - val_loss: 1.7595\n",
      "Epoch 18/100\n",
      "\u001b[1m125/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2849 - loss: 1.9318\n",
      "Epoch 18: val_loss improved from 1.75954 to 1.72656, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.2859 - loss: 1.9303 - val_accuracy: 0.4006 - val_loss: 1.7266\n",
      "Epoch 19/100\n",
      "\u001b[1m133/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3264 - loss: 1.8928\n",
      "Epoch 19: val_loss improved from 1.72656 to 1.66071, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3264 - loss: 1.8920 - val_accuracy: 0.3983 - val_loss: 1.6607\n",
      "Epoch 20/100\n",
      "\u001b[1m122/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3383 - loss: 1.8295\n",
      "Epoch 20: val_loss improved from 1.66071 to 1.62831, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3391 - loss: 1.8277 - val_accuracy: 0.4388 - val_loss: 1.6283\n",
      "Epoch 21/100\n",
      "\u001b[1m122/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3574 - loss: 1.7693\n",
      "Epoch 21: val_loss improved from 1.62831 to 1.56944, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3580 - loss: 1.7682 - val_accuracy: 0.4732 - val_loss: 1.5694\n",
      "Epoch 22/100\n",
      "\u001b[1m118/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3839 - loss: 1.7379\n",
      "Epoch 22: val_loss improved from 1.56944 to 1.53044, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3836 - loss: 1.7389 - val_accuracy: 0.5023 - val_loss: 1.5304\n",
      "Epoch 23/100\n",
      "\u001b[1m118/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3956 - loss: 1.7180\n",
      "Epoch 23: val_loss improved from 1.53044 to 1.49903, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3955 - loss: 1.7168 - val_accuracy: 0.4966 - val_loss: 1.4990\n",
      "Epoch 24/100\n",
      "\u001b[1m128/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3939 - loss: 1.6955\n",
      "Epoch 24: val_loss improved from 1.49903 to 1.46157, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3945 - loss: 1.6934 - val_accuracy: 0.5062 - val_loss: 1.4616\n",
      "Epoch 25/100\n",
      "\u001b[1m131/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4280 - loss: 1.6156\n",
      "Epoch 25: val_loss improved from 1.46157 to 1.44283, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4276 - loss: 1.6172 - val_accuracy: 0.5275 - val_loss: 1.4428\n",
      "Epoch 26/100\n",
      "\u001b[1m127/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4181 - loss: 1.6445\n",
      "Epoch 26: val_loss improved from 1.44283 to 1.41078, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4185 - loss: 1.6430 - val_accuracy: 0.5300 - val_loss: 1.4108\n",
      "Epoch 27/100\n",
      "\u001b[1m136/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4225 - loss: 1.6076\n",
      "Epoch 27: val_loss improved from 1.41078 to 1.39381, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4225 - loss: 1.6077 - val_accuracy: 0.5263 - val_loss: 1.3938\n",
      "Epoch 28/100\n",
      "\u001b[1m116/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4375 - loss: 1.5860\n",
      "Epoch 28: val_loss improved from 1.39381 to 1.35532, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4374 - loss: 1.5850 - val_accuracy: 0.5433 - val_loss: 1.3553\n",
      "Epoch 29/100\n",
      "\u001b[1m120/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4545 - loss: 1.5333\n",
      "Epoch 29: val_loss improved from 1.35532 to 1.34187, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4550 - loss: 1.5364 - val_accuracy: 0.5495 - val_loss: 1.3419\n",
      "Epoch 30/100\n",
      "\u001b[1m128/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4712 - loss: 1.5034\n",
      "Epoch 30: val_loss improved from 1.34187 to 1.30304, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4712 - loss: 1.5036 - val_accuracy: 0.5749 - val_loss: 1.3030\n",
      "Epoch 31/100\n",
      "\u001b[1m134/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4890 - loss: 1.4777\n",
      "Epoch 31: val_loss did not improve from 1.30304\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4888 - loss: 1.4782 - val_accuracy: 0.5593 - val_loss: 1.3265\n",
      "Epoch 32/100\n",
      "\u001b[1m132/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4665 - loss: 1.4811\n",
      "Epoch 32: val_loss improved from 1.30304 to 1.28759, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4666 - loss: 1.4812 - val_accuracy: 0.5877 - val_loss: 1.2876\n",
      "Epoch 33/100\n",
      "\u001b[1m134/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4838 - loss: 1.4783\n",
      "Epoch 33: val_loss improved from 1.28759 to 1.26884, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4839 - loss: 1.4779 - val_accuracy: 0.5918 - val_loss: 1.2688\n",
      "Epoch 34/100\n",
      "\u001b[1m100/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5034 - loss: 1.4527\n",
      "Epoch 34: val_loss improved from 1.26884 to 1.24551, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5001 - loss: 1.4532 - val_accuracy: 0.5896 - val_loss: 1.2455\n",
      "Epoch 35/100\n",
      "\u001b[1m124/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4934 - loss: 1.4503\n",
      "Epoch 35: val_loss improved from 1.24551 to 1.21102, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4944 - loss: 1.4488 - val_accuracy: 0.6083 - val_loss: 1.2110\n",
      "Epoch 36/100\n",
      "\u001b[1m128/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4951 - loss: 1.4266\n",
      "Epoch 36: val_loss improved from 1.21102 to 1.20200, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4963 - loss: 1.4243 - val_accuracy: 0.6109 - val_loss: 1.2020\n",
      "Epoch 37/100\n",
      "\u001b[1m112/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5039 - loss: 1.4095\n",
      "Epoch 37: val_loss improved from 1.20200 to 1.17926, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5058 - loss: 1.4074 - val_accuracy: 0.6028 - val_loss: 1.1793\n",
      "Epoch 38/100\n",
      "\u001b[1m110/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5173 - loss: 1.3970\n",
      "Epoch 38: val_loss did not improve from 1.17926\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5173 - loss: 1.3941 - val_accuracy: 0.6015 - val_loss: 1.2207\n",
      "Epoch 39/100\n",
      "\u001b[1m133/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5157 - loss: 1.3688\n",
      "Epoch 39: val_loss did not improve from 1.17926\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5158 - loss: 1.3686 - val_accuracy: 0.6086 - val_loss: 1.1849\n",
      "Epoch 40/100\n",
      "\u001b[1m127/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5308 - loss: 1.3429\n",
      "Epoch 40: val_loss improved from 1.17926 to 1.14541, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5310 - loss: 1.3425 - val_accuracy: 0.6267 - val_loss: 1.1454\n",
      "Epoch 41/100\n",
      "\u001b[1m135/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5336 - loss: 1.3375\n",
      "Epoch 41: val_loss improved from 1.14541 to 1.13099, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5336 - loss: 1.3376 - val_accuracy: 0.6322 - val_loss: 1.1310\n",
      "Epoch 42/100\n",
      "\u001b[1m132/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5252 - loss: 1.3399\n",
      "Epoch 42: val_loss improved from 1.13099 to 1.09814, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5261 - loss: 1.3387 - val_accuracy: 0.6379 - val_loss: 1.0981\n",
      "Epoch 43/100\n",
      "\u001b[1m105/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5483 - loss: 1.3348\n",
      "Epoch 43: val_loss improved from 1.09814 to 1.08208, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5493 - loss: 1.3293 - val_accuracy: 0.6441 - val_loss: 1.0821\n",
      "Epoch 44/100\n",
      "\u001b[1m131/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5399 - loss: 1.2994\n",
      "Epoch 44: val_loss did not improve from 1.08208\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5401 - loss: 1.2989 - val_accuracy: 0.6356 - val_loss: 1.0914\n",
      "Epoch 45/100\n",
      "\u001b[1m123/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5786 - loss: 1.2270\n",
      "Epoch 45: val_loss did not improve from 1.08208\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5769 - loss: 1.2309 - val_accuracy: 0.6475 - val_loss: 1.0962\n",
      "Epoch 46/100\n",
      "\u001b[1m123/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5908 - loss: 1.2273\n",
      "Epoch 46: val_loss improved from 1.08208 to 1.06035, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5895 - loss: 1.2296 - val_accuracy: 0.6541 - val_loss: 1.0603\n",
      "Epoch 47/100\n",
      "\u001b[1m122/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5754 - loss: 1.2376\n",
      "Epoch 47: val_loss improved from 1.06035 to 1.05161, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5746 - loss: 1.2383 - val_accuracy: 0.6628 - val_loss: 1.0516\n",
      "Epoch 48/100\n",
      "\u001b[1m128/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5992 - loss: 1.1877\n",
      "Epoch 48: val_loss improved from 1.05161 to 1.03955, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5992 - loss: 1.1892 - val_accuracy: 0.6647 - val_loss: 1.0395\n",
      "Epoch 49/100\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5748 - loss: 1.2495\n",
      "Epoch 49: val_loss did not improve from 1.03955\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5748 - loss: 1.2495 - val_accuracy: 0.6553 - val_loss: 1.0565\n",
      "Epoch 50/100\n",
      "\u001b[1m105/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5796 - loss: 1.2163\n",
      "Epoch 50: val_loss improved from 1.03955 to 1.03812, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5790 - loss: 1.2170 - val_accuracy: 0.6702 - val_loss: 1.0381\n",
      "Epoch 51/100\n",
      "\u001b[1m114/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5896 - loss: 1.1888\n",
      "Epoch 51: val_loss improved from 1.03812 to 0.99310, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5906 - loss: 1.1876 - val_accuracy: 0.6816 - val_loss: 0.9931\n",
      "Epoch 52/100\n",
      "\u001b[1m130/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5897 - loss: 1.2029\n",
      "Epoch 52: val_loss did not improve from 0.99310\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5897 - loss: 1.2036 - val_accuracy: 0.6704 - val_loss: 1.0236\n",
      "Epoch 53/100\n",
      "\u001b[1m130/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6092 - loss: 1.1772\n",
      "Epoch 53: val_loss did not improve from 0.99310\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6090 - loss: 1.1773 - val_accuracy: 0.6876 - val_loss: 0.9994\n",
      "Epoch 54/100\n",
      "\u001b[1m125/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6029 - loss: 1.1751\n",
      "Epoch 54: val_loss did not improve from 0.99310\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6023 - loss: 1.1757 - val_accuracy: 0.6709 - val_loss: 0.9962\n",
      "Epoch 55/100\n",
      "\u001b[1m132/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6019 - loss: 1.1504\n",
      "Epoch 55: val_loss improved from 0.99310 to 0.97554, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6019 - loss: 1.1502 - val_accuracy: 0.6853 - val_loss: 0.9755\n",
      "Epoch 56/100\n",
      "\u001b[1m124/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6099 - loss: 1.1393\n",
      "Epoch 56: val_loss did not improve from 0.97554\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6100 - loss: 1.1405 - val_accuracy: 0.6823 - val_loss: 1.0127\n",
      "Epoch 57/100\n",
      "\u001b[1m113/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6277 - loss: 1.1256\n",
      "Epoch 57: val_loss improved from 0.97554 to 0.96786, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6254 - loss: 1.1290 - val_accuracy: 0.6915 - val_loss: 0.9679\n",
      "Epoch 58/100\n",
      "\u001b[1m106/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5915 - loss: 1.1718\n",
      "Epoch 58: val_loss did not improve from 0.96786\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5931 - loss: 1.1710 - val_accuracy: 0.6926 - val_loss: 0.9715\n",
      "Epoch 59/100\n",
      "\u001b[1m126/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6311 - loss: 1.0887\n",
      "Epoch 59: val_loss improved from 0.96786 to 0.95294, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6299 - loss: 1.0915 - val_accuracy: 0.7068 - val_loss: 0.9529\n",
      "Epoch 60/100\n",
      "\u001b[1m122/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6232 - loss: 1.1068\n",
      "Epoch 60: val_loss improved from 0.95294 to 0.93305, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6219 - loss: 1.1104 - val_accuracy: 0.7107 - val_loss: 0.9331\n",
      "Epoch 61/100\n",
      "\u001b[1m130/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6083 - loss: 1.1305\n",
      "Epoch 61: val_loss did not improve from 0.93305\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6083 - loss: 1.1306 - val_accuracy: 0.7048 - val_loss: 0.9522\n",
      "Epoch 62/100\n",
      "\u001b[1m124/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6056 - loss: 1.1317\n",
      "Epoch 62: val_loss did not improve from 0.93305\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6058 - loss: 1.1319 - val_accuracy: 0.6981 - val_loss: 0.9499\n",
      "Epoch 63/100\n",
      "\u001b[1m130/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6138 - loss: 1.0930\n",
      "Epoch 63: val_loss did not improve from 0.93305\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6140 - loss: 1.0939 - val_accuracy: 0.6972 - val_loss: 0.9348\n",
      "Epoch 64/100\n",
      "\u001b[1m127/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6259 - loss: 1.0756\n",
      "Epoch 64: val_loss improved from 0.93305 to 0.91039, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6253 - loss: 1.0770 - val_accuracy: 0.7116 - val_loss: 0.9104\n",
      "Epoch 65/100\n",
      "\u001b[1m125/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6219 - loss: 1.1201\n",
      "Epoch 65: val_loss did not improve from 0.91039\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6228 - loss: 1.1170 - val_accuracy: 0.7107 - val_loss: 0.9167\n",
      "Epoch 66/100\n",
      "\u001b[1m136/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6206 - loss: 1.0860\n",
      "Epoch 66: val_loss did not improve from 0.91039\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6207 - loss: 1.0859 - val_accuracy: 0.7139 - val_loss: 0.9204\n",
      "Epoch 67/100\n",
      "\u001b[1m129/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6507 - loss: 1.0379\n",
      "Epoch 67: val_loss did not improve from 0.91039\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6506 - loss: 1.0389 - val_accuracy: 0.7009 - val_loss: 0.9250\n",
      "Epoch 68/100\n",
      "\u001b[1m123/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6499 - loss: 1.0487\n",
      "Epoch 68: val_loss did not improve from 0.91039\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6491 - loss: 1.0505 - val_accuracy: 0.7130 - val_loss: 0.9127\n",
      "Epoch 69/100\n",
      "\u001b[1m113/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6257 - loss: 1.0849\n",
      "Epoch 69: val_loss improved from 0.91039 to 0.90019, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6273 - loss: 1.0814 - val_accuracy: 0.7178 - val_loss: 0.9002\n",
      "Epoch 70/100\n",
      "\u001b[1m124/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6333 - loss: 1.0657\n",
      "Epoch 70: val_loss did not improve from 0.90019\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6333 - loss: 1.0665 - val_accuracy: 0.7057 - val_loss: 0.9280\n",
      "Epoch 71/100\n",
      "\u001b[1m130/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6481 - loss: 1.0541\n",
      "Epoch 71: val_loss improved from 0.90019 to 0.89107, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6477 - loss: 1.0542 - val_accuracy: 0.7180 - val_loss: 0.8911\n",
      "Epoch 72/100\n",
      "\u001b[1m119/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6493 - loss: 1.0242\n",
      "Epoch 72: val_loss improved from 0.89107 to 0.87146, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6496 - loss: 1.0261 - val_accuracy: 0.7226 - val_loss: 0.8715\n",
      "Epoch 73/100\n",
      "\u001b[1m132/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6371 - loss: 1.0597\n",
      "Epoch 73: val_loss did not improve from 0.87146\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6374 - loss: 1.0594 - val_accuracy: 0.7096 - val_loss: 0.9096\n",
      "Epoch 74/100\n",
      "\u001b[1m120/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6504 - loss: 1.0011\n",
      "Epoch 74: val_loss did not improve from 0.87146\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6491 - loss: 1.0051 - val_accuracy: 0.7167 - val_loss: 0.8818\n",
      "Epoch 75/100\n",
      "\u001b[1m118/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6525 - loss: 1.0561\n",
      "Epoch 75: val_loss did not improve from 0.87146\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6521 - loss: 1.0540 - val_accuracy: 0.7164 - val_loss: 0.8829\n",
      "Epoch 76/100\n",
      "\u001b[1m111/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6412 - loss: 1.0815\n",
      "Epoch 76: val_loss did not improve from 0.87146\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6417 - loss: 1.0770 - val_accuracy: 0.7247 - val_loss: 0.8790\n",
      "Epoch 77/100\n",
      "\u001b[1m125/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6459 - loss: 1.0662\n",
      "Epoch 77: val_loss improved from 0.87146 to 0.85128, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6462 - loss: 1.0641 - val_accuracy: 0.7171 - val_loss: 0.8513\n",
      "Epoch 78/100\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6572 - loss: 1.0385\n",
      "Epoch 78: val_loss did not improve from 0.85128\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6572 - loss: 1.0385 - val_accuracy: 0.7224 - val_loss: 0.8766\n",
      "Epoch 79/100\n",
      "\u001b[1m135/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6227 - loss: 1.0496\n",
      "Epoch 79: val_loss did not improve from 0.85128\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6230 - loss: 1.0492 - val_accuracy: 0.7146 - val_loss: 0.8883\n",
      "Epoch 80/100\n",
      "\u001b[1m123/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6591 - loss: 1.0259\n",
      "Epoch 80: val_loss did not improve from 0.85128\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6587 - loss: 1.0258 - val_accuracy: 0.7217 - val_loss: 0.8664\n",
      "Epoch 81/100\n",
      "\u001b[1m130/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6540 - loss: 1.0132\n",
      "Epoch 81: val_loss did not improve from 0.85128\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6543 - loss: 1.0140 - val_accuracy: 0.7268 - val_loss: 0.8722\n",
      "Epoch 82/100\n",
      "\u001b[1m121/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6435 - loss: 1.0305\n",
      "Epoch 82: val_loss improved from 0.85128 to 0.84885, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6442 - loss: 1.0304 - val_accuracy: 0.7290 - val_loss: 0.8489\n",
      "Epoch 83/100\n",
      "\u001b[1m134/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6736 - loss: 0.9804\n",
      "Epoch 83: val_loss improved from 0.84885 to 0.83916, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6734 - loss: 0.9806 - val_accuracy: 0.7352 - val_loss: 0.8392\n",
      "Epoch 84/100\n",
      "\u001b[1m115/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6533 - loss: 1.0066\n",
      "Epoch 84: val_loss improved from 0.83916 to 0.83902, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6536 - loss: 1.0072 - val_accuracy: 0.7391 - val_loss: 0.8390\n",
      "Epoch 85/100\n",
      "\u001b[1m120/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6567 - loss: 0.9955\n",
      "Epoch 85: val_loss did not improve from 0.83902\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6574 - loss: 0.9962 - val_accuracy: 0.7302 - val_loss: 0.8540\n",
      "Epoch 86/100\n",
      "\u001b[1m132/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6713 - loss: 1.0190\n",
      "Epoch 86: val_loss improved from 0.83902 to 0.82413, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6713 - loss: 1.0183 - val_accuracy: 0.7460 - val_loss: 0.8241\n",
      "Epoch 87/100\n",
      "\u001b[1m118/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6774 - loss: 1.0007\n",
      "Epoch 87: val_loss did not improve from 0.82413\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6751 - loss: 1.0045 - val_accuracy: 0.7295 - val_loss: 0.8534\n",
      "Epoch 88/100\n",
      "\u001b[1m124/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6696 - loss: 0.9509\n",
      "Epoch 88: val_loss did not improve from 0.82413\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6693 - loss: 0.9535 - val_accuracy: 0.7444 - val_loss: 0.8330\n",
      "Epoch 89/100\n",
      "\u001b[1m123/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6643 - loss: 0.9767\n",
      "Epoch 89: val_loss did not improve from 0.82413\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6630 - loss: 0.9805 - val_accuracy: 0.7377 - val_loss: 0.8522\n",
      "Epoch 90/100\n",
      "\u001b[1m135/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6639 - loss: 0.9809\n",
      "Epoch 90: val_loss did not improve from 0.82413\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6640 - loss: 0.9808 - val_accuracy: 0.7359 - val_loss: 0.8254\n",
      "Epoch 91/100\n",
      "\u001b[1m128/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6648 - loss: 1.0134\n",
      "Epoch 91: val_loss improved from 0.82413 to 0.82365, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6648 - loss: 1.0135 - val_accuracy: 0.7375 - val_loss: 0.8237\n",
      "Epoch 92/100\n",
      "\u001b[1m114/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6714 - loss: 0.9780\n",
      "Epoch 92: val_loss improved from 0.82365 to 0.82340, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6700 - loss: 0.9795 - val_accuracy: 0.7430 - val_loss: 0.8234\n",
      "Epoch 93/100\n",
      "\u001b[1m136/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6712 - loss: 0.9686\n",
      "Epoch 93: val_loss improved from 0.82340 to 0.81888, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6712 - loss: 0.9685 - val_accuracy: 0.7412 - val_loss: 0.8189\n",
      "Epoch 94/100\n",
      "\u001b[1m135/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6584 - loss: 1.0064\n",
      "Epoch 94: val_loss improved from 0.81888 to 0.81015, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6585 - loss: 1.0058 - val_accuracy: 0.7375 - val_loss: 0.8101\n",
      "Epoch 95/100\n",
      "\u001b[1m134/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6711 - loss: 0.9583\n",
      "Epoch 95: val_loss improved from 0.81015 to 0.80916, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6710 - loss: 0.9594 - val_accuracy: 0.7536 - val_loss: 0.8092\n",
      "Epoch 96/100\n",
      "\u001b[1m118/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6742 - loss: 0.9860\n",
      "Epoch 96: val_loss improved from 0.80916 to 0.78965, saving model to saved_models/audio_classification.keras\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6740 - loss: 0.9828 - val_accuracy: 0.7561 - val_loss: 0.7896\n",
      "Epoch 97/100\n",
      "\u001b[1m131/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6589 - loss: 0.9931\n",
      "Epoch 97: val_loss did not improve from 0.78965\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6593 - loss: 0.9922 - val_accuracy: 0.7460 - val_loss: 0.8059\n",
      "Epoch 98/100\n",
      "\u001b[1m115/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6648 - loss: 0.9890\n",
      "Epoch 98: val_loss did not improve from 0.78965\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6661 - loss: 0.9862 - val_accuracy: 0.7464 - val_loss: 0.7944\n",
      "Epoch 99/100\n",
      "\u001b[1m129/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6813 - loss: 0.9785\n",
      "Epoch 99: val_loss did not improve from 0.78965\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6813 - loss: 0.9779 - val_accuracy: 0.7558 - val_loss: 0.8128\n",
      "Epoch 100/100\n",
      "\u001b[1m114/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6945 - loss: 0.9223\n",
      "Epoch 100: val_loss did not improve from 0.78965\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6919 - loss: 0.9250 - val_accuracy: 0.7430 - val_loss: 0.8082\n",
      "Training completed in time:  0:01:01.109904\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.keras', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71a09e65-18c8-4ddc-9731-6d296383f3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7430142164230347\n"
     ]
    }
   ],
   "source": [
    "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9127aa3b-3a7e-494e-8518-64e439d35874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "['gun_shot']\n"
     ]
    }
   ],
   "source": [
    "filename = \"UrbanSound8K/9031-3-1-0.wav\"\n",
    "mfccs_scaled_features = features_extractor(filename).reshape(1, -1)\n",
    "\n",
    "# Predict the class probabilities\n",
    "predicted_label = model.predict(mfccs_scaled_features)\n",
    "\n",
    "# Convert probabilities to class index\n",
    "predicted_class_index = np.argmax(predicted_label, axis=1)\n",
    "\n",
    "# Inverse transform to get original class label\n",
    "prediction_class = labelencoder.inverse_transform(predicted_class_index)\n",
    "print(prediction_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c42a1-cbd0-4642-9d26-2c7548ec72a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
